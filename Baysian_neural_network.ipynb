{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabber/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import edward as ed\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from edward.models import Categorical, Normal\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sklearn related modules\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by http://nipunbatra.github.io/2014/08/latexify/\n",
    "plt.style.use(['default'])\n",
    "params = {\n",
    "    'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n",
    "    'image.origin': 'lower',\n",
    "    'image.interpolation': 'nearest',\n",
    "    'image.cmap': 'gray',\n",
    "    'axes.grid': False,\n",
    "    'savefig.dpi': 150,  # to adjust notebook inline plot size\n",
    "    'axes.labelsize': 12, # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize': 12,\n",
    "    'font.size': 12, # was 10\n",
    "    'legend.fontsize': 12, # was 10\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'text.usetex': True,\n",
    "    'figure.facecolor':'white',\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look data with pandas\n",
    "train_file = \"data/rupturemodel_train.txt\"\n",
    "val_file = \"data/rupturemodel_xvalidate.txt\"\n",
    "test_file = \"data/rupturemodel_test.txt\"\n",
    "\n",
    "df_train= pd.read_csv(train_file, sep=\" \", header = None, dtype = np.float32)\n",
    "df_val= pd.read_csv(val_file, sep=\" \", header = None, dtype = np.float32)\n",
    "df_test= pd.read_csv(test_file, sep=\" \", header = None, dtype = np.float32)\n",
    "\n",
    "features_name =  ['height', 'width', 'sxx', 'sxy', 'syy', 'sdrop', 'mud', 'dc', 'label']\n",
    "df_train.columns = features_name\n",
    "df_val.columns = features_name\n",
    "df_test.columns = features_name\n",
    "\n",
    "df_train_all = df_train.append(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "We use StandardScaler preprocessing method of scikit. It standardizes features \n",
    "by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df_train_all, random_state = random_seed)\n",
    "df_train = shuffle(df_train, random_state = random_seed)\n",
    "df_val = shuffle(df_val, random_state = random_seed)\n",
    "df_test = shuffle(df_test, random_state = random_seed)\n",
    "\n",
    "mms = StandardScaler()\n",
    "train_scale = mms.fit(df.drop('label', axis=1))\n",
    "\n",
    "xtrain = train_scale.transform(df.drop('label', axis=1))\n",
    "ytrain =  np.array(df['label'].values)\n",
    "ytrain = ytrain.reshape(ytrain.shape[0], 1)\n",
    "\n",
    "xtest = train_scale.transform(df_test.drop('label', axis=1))\n",
    "ytest = np.array(df_test['label'].values)\n",
    "ytest = ytest.reshape(ytest.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prior weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "N = np.shape(xtrain)[0]\n",
    "in_size = np.shape(xtrain)[1]\n",
    "out_size = 1\n",
    "l1 = 12 # Number of neurons in the first layer\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, W_0, W_1, b_0, b_1):\n",
    "    hidden1 = tf.nn.relu(tf.matmul(X, W_0) + b_0)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden1, W_1) + b_1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('model_prior'):\n",
    "    \n",
    "    W_0 = Normal(loc = tf.zeros([in_size, l1], name=\"weights/loc_w_0\"), \n",
    "                 scale=tf.ones([in_size, l1], name=\"weights/scale_w_0\"),\n",
    "                 name  = 'weight_w_0')\n",
    "    W_1 = Normal(loc=tf.zeros([l1, out_size], name=\"weights/loc_w_1\"), \n",
    "                 scale=tf.ones([l1, out_size], name=\"weights/scale_w_1\"),\n",
    "                 name  = 'weight_w_1')\n",
    "    b_0 = Normal(loc=tf.zeros(l1, name=\"weights/loc_b_0\"), \n",
    "                 scale=tf.ones(l1, name=\"weights/scale_b_0\"),\n",
    "                 name  = 'weight_b_0')\n",
    "    b_1 = Normal(loc=tf.zeros(out_size, name=\"weights/loc_b_1\"),\n",
    "                 scale=tf.ones(out_size, name=\"weights/scale_b_1\"),\n",
    "                 name  = 'weight_b_1')\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = (None, in_size)) \n",
    "    y = Normal(loc = neural_network(X, W_0, W_1, b_0, b_1), scale = 1.0)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('model_prior'):\n",
    "    \n",
    "    W_0 = Normal(loc = tf.zeros([in_size, l1], name=\"weights/loc_w_0\"), \n",
    "                 scale=tf.ones([in_size, l1], name=\"weights/scale_w_0\"),\n",
    "                 name  = 'weight_w_0')\n",
    "    W_1 = Normal(loc=tf.zeros([l1, out_size], name=\"weights/loc_w_1\"), \n",
    "                 scale=tf.ones([l1, out_size], name=\"weights/scale_w_1\"),\n",
    "                 name  = 'weight_w_1')\n",
    "    b_0 = Normal(loc=tf.zeros(l1, name=\"weights/loc_b_0\"), \n",
    "                 scale=tf.ones(l1, name=\"weights/scale_b_0\"),\n",
    "                 name  = 'weight_b_0')\n",
    "    b_1 = Normal(loc=tf.zeros(out_size, name=\"weights/loc_b_1\"),\n",
    "                 scale=tf.ones(out_size, name=\"weights/scale_b_1\"),\n",
    "                 name  = 'weight_b_1')\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = (None, in_size)) \n",
    "    y = Normal(loc = neural_network(X, W_0, W_1, b_0, b_1), scale = 1.0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('posterior'):\n",
    "    qW_0 = Normal(loc=tf.Variable(tf.random_normal([in_size, l1]), name = \"weights/loc_qw_0\" ), \n",
    "                  scale=tf.Variable(tf.random_normal([in_size, l1]), name = \"weights/scale_qw_0\"),\n",
    "                  name = \"weight_qw_0\")\n",
    "    \n",
    "    qW_1 = Normal(loc=tf.Variable(tf.random_normal([l1, out_size]), name = \"weights/loc_qw_1\" ), \n",
    "                  scale=tf.Variable(tf.random_normal([l1, out_size]), name = \"weights/loc_qw_1\" ),\n",
    "                  name = \"weight_qw_1\")\n",
    "    \n",
    "    qb_0 = Normal(loc=tf.Variable(tf.random_normal([l1]), name = \"weights/loc_qb_0\" ), \n",
    "                scale=tf.Variable(tf.random_normal([l1]), name = \"weights/loc_qb_0\" ),\n",
    "                name = \"weight_qb_0\")\n",
    "    qb_1 = Normal(loc=tf.Variable(tf.random_normal([out_size]), name = \"weights/loc_qb_1\" ), \n",
    "                scale=tf.Variable(tf.random_normal([out_size]), name = \"weights/loc_qb_1\" ),\n",
    "                name = \"weight_qb_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabber/anaconda3/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [100%] ██████████████████████████████ Elapsed: 30s | Loss: 1667.624\n"
     ]
    }
   ],
   "source": [
    "inference = ed.KLqp({W_0: qW_0, b_0: qb_0,\n",
    "                     W_1: qW_1, b_1: qb_1}, \n",
    "                    data={X: xtrain, \n",
    "                          y: ytrain})\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.05\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           10000, 0.3, staircase=True)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "inference.run(n_iter=20000, optimizer=optimizer, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to sample...\n",
      "Took 15 secs.\n",
      "Sampling...\n",
      "Took 6 secs.\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "qW_0_samples = qW_0.sample(sample_shape=n_samples)\n",
    "qW_1_samples = qW_1.sample(sample_shape=n_samples)\n",
    "qb_0_samples = qb_0.sample(sample_shape=n_samples)\n",
    "qb_1_samples = qb_1.sample(sample_shape=n_samples)\n",
    "\n",
    "print(\"Preparing to sample...\")\n",
    "\n",
    "t0 = time()\n",
    "samplenodes = tf.stack([neural_network(X, qW_0_samples[i], qW_1_samples[i],\n",
    "                                       qb_0_samples[i], qb_1_samples[i]) \n",
    "                        for i in range(n_samples)], axis=0)\n",
    "\n",
    "print(\"Took\", np.int(time() - t0), \"secs.\")\n",
    "\n",
    "print(\"Sampling...\")\n",
    "t0 = time()\n",
    "samplepredictions = samplenodes.eval(feed_dict={X: xtest})\n",
    "print(\"Took\", np.int(time() - t0), \"secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "centers = []\n",
    "\n",
    "for i in range(len(xtest)):\n",
    "    histogram = np.histogram(samplepredictions[:,i], bins=20)\n",
    "    probs.append(histogram[0] / n_samples)\n",
    "    delta = histogram[1][1] - histogram[1][0]\n",
    "    centers.append([np.float32(a + delta / 2) for a in histogram[1][:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forming the posterior predictive distribution for test data point 1 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 2 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 3 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 4 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 5 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 6 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 7 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 8 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 9 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 10 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 11 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 12 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 13 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 14 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 15 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 16 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 17 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 18 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 19 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 20 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 21 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 22 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 23 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 24 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 25 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 26 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 27 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 28 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 29 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 30 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 31 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 32 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 33 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 34 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 35 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 36 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 37 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 38 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 39 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 40 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 41 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 42 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 43 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 44 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 45 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 46 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 47 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 48 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 49 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 50 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 51 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 52 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 53 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 54 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 55 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 56 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 57 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 58 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 59 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 60 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 61 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 62 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 63 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 64 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 65 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 66 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 67 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 68 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 69 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 70 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 71 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 72 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 73 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 74 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 75 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 76 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 77 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 78 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 79 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 80 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 81 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 82 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 83 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 84 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 85 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 86 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 87 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 88 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 89 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 90 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 91 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 92 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 93 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 94 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 95 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 96 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 97 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 98 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 99 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 100 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 101 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 102 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 103 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 104 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 105 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 106 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 107 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 108 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 109 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 110 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 111 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 112 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 113 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 114 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 115 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 116 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 117 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 118 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 119 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 120 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 121 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 122 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 123 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 124 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 125 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 126 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 127 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 128 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 129 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 130 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 131 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 132 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 133 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 134 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 135 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 136 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 137 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 138 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 139 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 140 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 141 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 142 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 143 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 144 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 145 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 146 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 147 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 148 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 149 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 150 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 151 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 152 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 153 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 154 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 155 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 156 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 157 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 158 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 159 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 160 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 161 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 162 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 163 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 164 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 165 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 166 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 167 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 168 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 169 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 170 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 171 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 172 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 173 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 174 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 175 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 176 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 177 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 178 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 179 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 180 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 181 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 182 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 183 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 184 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 185 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 186 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 187 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 188 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 189 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 190 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 191 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 192 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 193 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 194 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 195 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 196 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 197 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 198 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 199 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 200 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 201 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 202 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 203 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 204 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 205 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 206 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 207 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 208 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 209 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 210 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 211 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 212 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 213 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 214 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 215 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 216 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 217 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 218 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 219 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 220 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 221 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 222 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 223 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 224 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 225 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 226 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 227 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 228 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 229 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 230 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 231 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 232 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 233 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 234 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 235 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 236 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 237 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 238 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 239 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 240 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 241 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 242 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 243 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 244 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 245 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 246 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 247 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 248 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 249 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 250 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 251 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 252 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 253 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 254 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 255 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 256 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 257 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 258 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 259 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 260 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 261 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 262 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 263 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 264 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 265 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 266 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 267 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 268 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 269 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 270 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 271 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 272 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 273 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 274 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 275 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 276 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 277 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 278 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 279 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 280 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 281 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 282 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 283 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 284 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 285 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 286 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 287 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 288 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 289 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 290 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 291 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 292 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 293 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 294 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 295 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 296 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 297 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 298 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 299 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 300 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 301 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 302 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 303 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 304 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 305 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 306 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 307 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 308 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 309 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 310 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 311 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 312 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 313 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 314 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 315 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 316 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 317 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 318 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 319 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 320 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 321 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 322 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 323 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 324 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 325 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 326 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 327 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 328 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 329 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 330 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 331 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 332 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 333 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 334 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 335 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 336 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 337 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 338 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 339 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 340 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 341 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 342 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 343 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 344 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 345 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 346 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 347 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 348 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 349 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 350 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 351 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 352 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 353 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 354 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 355 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 356 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 357 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 358 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 359 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 360 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 361 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 362 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 363 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 364 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 365 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 366 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 367 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 368 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 369 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 370 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 371 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 372 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 373 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 374 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 375 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 376 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 377 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 378 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 379 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 380 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 381 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 382 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 383 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 384 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 385 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 386 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 387 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 388 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 389 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 390 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 391 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 392 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 393 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 394 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 395 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 396 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 397 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 398 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 399 / 400 ...\n",
      "Forming the posterior predictive distribution for test data point 400 / 400 ...\n",
      "Took altogether 386 secs.\n"
     ]
    }
   ],
   "source": [
    "from edward.models import Categorical, Mixture\n",
    "y_post = []\n",
    "\n",
    "t0 = time()\n",
    "for i in range(len(xtest)):\n",
    "    print(\"Forming the posterior predictive distribution for test data point\", i+1, \"/\", len(xtest), \"...\")\n",
    "    y_post.append(Mixture(Categorical(probs = probs[i]), \n",
    "                  [Normal(loc=centers[i][j], scale=1.0) for j in range(len(centers[i]))]))\n",
    "    \n",
    "print(\"Took altogether\", np.int(time() - t0), \"secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling the posterior predictive distribution for 400 test data points...\n",
      "Took 2315 secs.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "print(\"Sampling the posterior predictive distribution for\", len(xtest), \"test data points...\")\n",
    "posteriorsamplenodes = tf.stack([y_post[i].sample(n_samples) for i in range(len(xtest))], axis=1)\n",
    "posteriorsamples = pd.DataFrame(posteriorsamplenodes.eval())\n",
    "print(\"Took\", np.int(time() - t0), \"secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = posteriorsamples.mean()\n",
    "predictions_low = posteriorsamples.quantile(0.01)\n",
    "predictions_high = posteriorsamples.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = []\n",
    "for pred in predictions:\n",
    "    if pred > 0.5:\n",
    "        ypred.append(1)\n",
    "    else:\n",
    "        ypred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(ytest, ypred):\n",
    "    print('Accuracy of the model: {}\\n'.format(accuracy_score(ytest, ypred)))\n",
    "    print('Classification report: \\n{}\\n'.format(classification_report(ytest, ypred)))\n",
    "    print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(ytest, ypred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.8225\n",
      "\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.93      0.88       272\n",
      "        1.0       0.80      0.59      0.68       128\n",
      "\n",
      "avg / total       0.82      0.82      0.81       400\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[253  19]\n",
      " [ 52  76]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Generate samples the posterior and store them.\n",
    "n_samples = 100\n",
    "prob_lst = []\n",
    "samples = []\n",
    "\n",
    "w_0_samples = []\n",
    "b_0_samples = []\n",
    "w_1_samples = []\n",
    "b_1_samples = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    w_0_samp = qW_0.sample()\n",
    "    b_0_samp = qb_0.sample()\n",
    "    w_1_samp = qW_1.sample()\n",
    "    b_1_samp = qb_1.sample()\n",
    "    \n",
    "    w_0_samples.append(w_0_samp)\n",
    "    w_1_samples.append(w_1_samp)\n",
    "    b_0_samples.append(b_0_samp)\n",
    "    b_1_samples.append(b_1_samp)\n",
    "    \n",
    "    # prediction on test data\n",
    "    h = tf.nn.relu(tf.matmul(xtest.astype('float32'), w_0_samp) + b_0_samp)\n",
    "    y_pred = tf.nn.sigmoid(tf.matmul(h, w_1_samp) + b_1_samp)\n",
    "    \n",
    "    ## Probability calculation using softmax\n",
    "    prob = tf.nn.softmax(y_pred)\n",
    "    prob_lst.append(y_pred.eval())\n",
    "    \n",
    "#     sample = tf.concat([tf.reshape(w_samp,[-1]), b_samp],0)\n",
    "#     samples.append(sample.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(prob_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accy_test = []\n",
    "for prob in prob_lst:\n",
    "    y_trn_prd = np.argmax(prob, axis=1).astype(np.float32)\n",
    "    ytest2 = np.argmax(ytest, axis=1).astype(np.float32)\n",
    "    \n",
    "    acc = (y_trn_prd == ytest2).mean()*100\n",
    "    accy_test.append(acc)\n",
    "\n",
    "plt.hist(accy_test)\n",
    "plt.title(\"Histogram of prediction accuracies in the MNIST test data\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
